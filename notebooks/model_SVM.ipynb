{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46ea179d-14bd-406e-980c-d80773eab430",
   "metadata": {},
   "source": [
    "# Support Vector Machine (SVM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "308d55cf-2ecd-4f83-a00b-45dbea1f42ef",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6d91c7-6610-44a2-b0de-d0ee1991f659",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import posixpath\n",
    "from data_mining_project import data, PROJECT_PATH, DATA_PATH, OUTPUT_PATH\n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "import plotly.express as px\n",
    "import time\n",
    "from sklearn.svm import SVC\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be50830-1ab5-4739-a5be-16be73b85c1c",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be71dfe-1fa4-4772-9c4d-24883392f158",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"preprocessed_data.csv\"  \n",
    "file_path = posixpath.join(OUTPUT_PATH, file_name)\n",
    "data_df = data.load_data_csv(file_path)\n",
    "data_df = data.reformat_str_to_list(data_df, cols=[\"events_sequence\", \"seconds_to_incident_sequence\", \"dj_ac_state_sequence\", \"dj_dc_state_sequence\", \"ac_dc_prob_timestamp\"], col_type=int)\n",
    "data_df = data.reformat_str_to_list(data_df, cols=[\"train_kph_sequence\"], col_type=float)\n",
    "#data_df.drop(columns=[\"ac_dc_prob_num\", \"ac_dc_prob\"], inplace=True)\n",
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d16edba-3be1-4685-a5ef-25dd6ee1f866",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization to obtain relative frequencies\n",
    "braking = data_df[data_df['hard_braking']==1]\n",
    "brake_counts = braking[\"incident_type\"].value_counts(normalize=True)\n",
    "data_df_counts = data_df[\"incident_type\"].value_counts(normalize=True)\n",
    "\n",
    "res = {}\n",
    "\n",
    "for id, val1 in brake_counts.items():\n",
    "    if id in data_df_counts:\n",
    "        val2 = data_df_counts[id]\n",
    "        res[id] = val1 / val2\n",
    "\n",
    "incident_type = []\n",
    "relative_probability = []\n",
    "for id, res in res.items():\n",
    "    incident_type.append(id)\n",
    "    relative_probability.append(res)\n",
    "    print(f\"Incident type {id}: Relative probability = {res:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48f1ba5-adc3-4f91-9f72-b2542596a00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "types_incidents_str = [str(t) for t in incident_type]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "colors = [\"green\" if p > 1 else \"red\" for p in relative_probability]\n",
    "plt.bar(types_incidents_str, relative_probability, color=colors, edgecolor=\"black\", alpha=0.7)\n",
    "\n",
    "plt.axhline(y=1, color='blue', linestyle='--', label=\"Relative probability = 1\")\n",
    "plt.xlabel(\"Incident type\", fontsize=12)\n",
    "plt.ylabel(\"Relative probability\", fontsize=12)\n",
    "plt.title(\"The impact of hard braking on incident types\", fontsize=14)\n",
    "plt.xticks(fontsize=10)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd88332-e4c8-4046-a994-0d6ca6fd78eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Input, Model\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras.layers import Embedding\n",
    "\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd2f5eb-2992-4f75-bf06-c0b9b1ad8e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "event_to_index = {event: idx for idx, event in enumerate(set([item for sublist in data_df['events_sequence'] for item in sublist]))}\n",
    "vocab_size = len(event_to_index)\n",
    "\n",
    "def map_sequence_to_indices(sequence, event_to_index):\n",
    "    return [event_to_index.get(event, event_to_index.get('unknown', -1)) for event in sequence] \n",
    "\n",
    "data_df['events_sequence_mapped'] = data_df['events_sequence'].apply(lambda seq: map_sequence_to_indices(seq, event_to_index))\n",
    "\n",
    "embedding_dim = 200 #first test : 100 \n",
    "\n",
    "input_seq = Input(shape=(None,), dtype='int32')\n",
    "embedding_layer = Embedding(input_dim=vocab_size, output_dim=embedding_dim)(input_seq)\n",
    "embedding_model = Model(inputs=input_seq, outputs=embedding_layer)\n",
    "\n",
    "def get_sequence_embeddings(df, embedding_model):\n",
    "    X = []\n",
    "    for _, row in df.iterrows():\n",
    "        event_indices = row['events_sequence_mapped']\n",
    "        embedded_seq = embedding_model.predict(np.array([event_indices]))  \n",
    "        sequence_embedding = np.mean(embedded_seq, axis=1) \n",
    "        X.append(sequence_embedding.flatten())\n",
    "    return np.array(X)\n",
    "\n",
    "data_df['hard_braking_numeric'] = data_df['hard_braking'].astype(int)\n",
    "\n",
    "X_embeddings = get_sequence_embeddings(data_df, embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0273534-8e68-4f79-9530-fbdb7c50a785",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_embeddings)\n",
    "X_combined = np.hstack([X_scaled, data_df[['ac_dc_prob']].values])\n",
    "\n",
    "y = data_df['incident_type'] \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_combined, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Entraîner un modèle SVM avec noyau RBF\n",
    "svm = SVC(kernel='rbf', decision_function_shape='ovr') #en ajoutant des param l'accuracy diminue\n",
    "\"\"\"C=10, gamma=0.01, class_weight='balanced',\"\"\"\n",
    "#svm = SVC(kernel='rbf', random_state=42)\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "y_pred = svm.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "#first test : 0.3435\n",
    "#plus de param et 300 : 0.32\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9384e35a-5127-43dc-a4f9-6c7a63e67ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d198c079-c6ad-4b0d-b67c-ab6595b85a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"que les séquences    0.3639\n",
    "    + hard braking      0.3537 + ac_dc             0.3639\n",
    "    + ac_dc             0.3639\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d428fe6-d5a8-494d-acb6-26f387c3d248",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(y_test).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf1ae36-0fc8-4cba-8e6e-9b96e63a4457",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(y_pred).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f95713-7d61-4f9b-8c8d-9fd81cbca7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df['events_sequence_mapped'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ae7495-5b3a-4c96-9d70-6024373f7f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting necessary details from the dataset and notebook for implementation\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Parameters for embedding generation\n",
    "embedding_sizes = [16, 32, 64, 128]  # Example embedding dimensions\n",
    "results = []\n",
    "\n",
    "# Synthetic data setup for demonstration (adjust as per actual sequence data)\n",
    "# Generating synthetic data assuming sequences are integer-encoded\n",
    "sequence_length = 50  # Example sequence length\n",
    "vocab_size = 1000  # Example vocab size\n",
    "synthetic_sequences = np.random.randint(0, vocab_size, (data_df.shape[0], sequence_length))\n",
    "\n",
    "# Iteratively test different embedding sizes\n",
    "for embedding_dim in embedding_sizes:\n",
    "    print(f\"Testing embedding dimension: {embedding_dim}\")\n",
    "    \n",
    "    # Generate synthetic embeddings for demonstration\n",
    "    X_embeddings = np.random.randn(data_df.shape[0], embedding_dim)  # Placeholder embedding\n",
    "    \n",
    "    # Combine embeddings with other features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X_embeddings)\n",
    "    X_combined = np.hstack([X_scaled, data_df[['ac_dc_prob']].values])\n",
    "    y = data_df['incident_type']\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_combined, y, test_size=0.3, random_state=42)\n",
    "    \n",
    "    # Train SVM model\n",
    "    svm = SVC(kernel='rbf', decision_function_shape='ovr', C=10, gamma=0.01, class_weight='balanced')\n",
    "    svm.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluate model\n",
    "    y_pred = svm.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Accuracy with embedding size {embedding_dim}: {accuracy:.4f}\")\n",
    "    results.append((embedding_dim, accuracy))\n",
    "\n",
    "# # Prepare results as a DataFrame for better visualization\n",
    "# import pandas as pd\n",
    "\n",
    "results_df = pd.DataFrame(results, columns=['Embedding Size', 'Accuracy'])\n",
    "# import ace_tools as tools; tools.display_dataframe_to_user(name=\"Embedding Length Tuning Results\", dataframe=results_df)\n",
    "# Display the results as a table without relying on the missing module\n",
    "print(results_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169eee7e-15ec-4917-b45a-a2af3420fcb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6f7b24-3a54-4be6-a421-16b6d2d6564d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# List of embedding dimensions to test\n",
    "embedding_lengths = [100, 125, 150, 175, 200, 300]\n",
    "\n",
    "# Store results\n",
    "results = []\n",
    "\n",
    "# Initialize lists to store additional metrics\n",
    "accuracy_scores = []\n",
    "f1_scores = []\n",
    "confusion_matrices = []\n",
    "\n",
    "for embedding_dim in embedding_lengths:\n",
    "    print(f\"Testing embedding length: {embedding_dim}\")\n",
    "    \n",
    "    # Define the embedding layer with the current embedding dimension\n",
    "    input_seq = Input(shape=(None,), dtype='int32')\n",
    "    embedding_layer = Embedding(input_dim=vocab_size, output_dim=embedding_dim)(input_seq)\n",
    "    embedding_model = Model(inputs=input_seq, outputs=embedding_layer)\n",
    "    \n",
    "    # Get sequence embeddings\n",
    "    X_embeddings = get_sequence_embeddings(data_df, embedding_model)\n",
    "    \n",
    "    # Scale embeddings\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X_embeddings)\n",
    "    \n",
    "    # Combine with additional features\n",
    "    X_combined = np.hstack([X_scaled, data_df[['ac_dc_prob']].values])\n",
    "    \n",
    "    # Define target variable\n",
    "    y = data_df['incident_type']\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_combined, y, test_size=0.3, random_state=42)\n",
    "    \n",
    "    # Train SVM model\n",
    "    svm = SVC(kernel='rbf', decision_function_shape='ovr', C=10, gamma=0.01, class_weight='balanced')\n",
    "    svm.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    y_pred = svm.predict(X_test)\n",
    "    report = classification_report(y_test, y_pred, output_dict=True)\n",
    "    \n",
    "    # Store metrics\n",
    "    accuracy = report['accuracy']\n",
    "    f1_score = report['weighted avg']['f1-score']\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    accuracy_scores.append(accuracy)\n",
    "    f1_scores.append(f1_score)\n",
    "    confusion_matrices.append(cm)\n",
    "    \n",
    "    print(f\"Accuracy with embedding length {embedding_dim}: {accuracy:.4f}\")\n",
    "    print(f\"F1 Score with embedding length {embedding_dim}: {f1_score:.4f}\")\n",
    "    results.append((embedding_dim, accuracy, f1_score))\n",
    "\n",
    "# Visualization\n",
    "# 1. Bar Chart for Accuracy and F1 Score\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar([str(dim) for dim in embedding_lengths], accuracy_scores, label='Accuracy')\n",
    "plt.bar([str(dim) for dim in embedding_lengths], f1_scores, alpha=0.7, label='F1 Score')\n",
    "plt.xlabel('Embedding Length')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Accuracy and F1 Score by Embedding Length')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Display results as a DataFrame\n",
    "results_df = pd.DataFrame(results, columns=['Embedding Length', 'Accuracy', 'F1 Score'])\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3dc6515-3787-41b9-acce-6f9af745f5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the best accuracy and F1-score points\n",
    "best_accuracy_index = accuracy_scores.index(max(accuracy_scores))\n",
    "best_f1_index = f1_scores.index(max(f1_scores))\n",
    "\n",
    "best_accuracy_dim = embedding_lengths[best_accuracy_index]\n",
    "best_accuracy = accuracy_scores[best_accuracy_index]\n",
    "\n",
    "best_f1_dim = embedding_lengths[best_f1_index]\n",
    "best_f1_score = f1_scores[best_f1_index]\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot accuracy\n",
    "plt.plot(embedding_lengths, accuracy_scores, marker='o', linestyle='-', color='green', label='Accuracy')\n",
    "\n",
    "# Plot F1-score\n",
    "plt.plot(embedding_lengths, f1_scores, marker='o', linestyle='-', color='blue', label='F1-Score')\n",
    "\n",
    "# Highlight the best accuracy and F1 points\n",
    "plt.scatter([best_accuracy_dim], [best_accuracy], color='red', label=f\"Best Accuracy (t={best_accuracy_dim}, Acc={best_accuracy:.3f})\")\n",
    "plt.text(best_accuracy_dim, best_accuracy, f\"({best_accuracy_dim}, {best_accuracy:.3f})\", fontsize=10)\n",
    "\n",
    "plt.scatter([best_f1_dim], [best_f1_score], color='orange', label=f\"Best F1-Score (t={best_f1_dim}, F1={best_f1_score:.3f})\")\n",
    "plt.text(best_f1_dim, best_f1_score, f\"({best_f1_dim}, {best_f1_score:.3f})\", fontsize=10)\n",
    "\n",
    "# Add labels, title, and legend\n",
    "plt.xlabel('Embedding Length')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Accuracy and F1-Score Trends by Embedding Length')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081eef80-6836-4933-8fb7-670a08b381e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract unique incident types and sort them\n",
    "unique_incident_types = data_df['incident_type'].unique()\n",
    "sorted_incident_types = sorted(unique_incident_types)\n",
    "\n",
    "# Update axis labels to reflect actual incident type numbers\n",
    "incident_type_labels = sorted_incident_types\n",
    "\n",
    "# Plot confusion matrices for each embedding length\n",
    "for i, embedding_dim in enumerate(embedding_lengths):\n",
    "    # Normalize the confusion matrix for readability\n",
    "    normalized_cm = confusion_matrices[i].astype('float') / confusion_matrices[i].sum(axis=1, keepdims=True)\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(\n",
    "        normalized_cm, \n",
    "        annot=True, \n",
    "        fmt='.2f', \n",
    "        cmap='Blues', \n",
    "        xticklabels=incident_type_labels,  # Use the actual incident type numbers as labels\n",
    "        yticklabels=incident_type_labels,\n",
    "        cbar=True\n",
    "    )\n",
    "    plt.title(f'Confusion Matrix for Embedding Length {embedding_dim}')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.xticks(rotation=0, ha='right')\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
